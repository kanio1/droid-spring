#!/bin/bash

################################################################################
# BSS Scalable Architecture Quick-Start Script
# Purpose: Setup 3-VM Proxmox environment for 400,000 events/minute
# Author: Expert Claude
# Date: 2025-11-07
################################################################################

set -e

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Configuration
VM_COUNT=3
TARGET_EVENTS_PER_MIN=400000
TARGET_EVENTS_PER_SEC=$((TARGET_EVENTS_PER_MIN / 60))

echo -e "${BLUE}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${BLUE}â•‘   BSS SCALABLE ARCHITECTURE - 400K EVENTS/MIN SETUP     â•‘${NC}"
echo -e "${BLUE}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "${YELLOW}Configuration:${NC}"
echo "  - Target: ${TARGET_EVENTS_PER_MIN:,} events/minute (${TARGET_EVENTS_PER_SEC} events/sec)"
echo "  - VMs: ${VM_COUNT}"
echo "  - PostgreSQL 18 + Citus"
echo "  - Redis 7.2 Cluster"
echo "  - Kafka 3.7 (KRaft)"
echo ""

# Check if running on Proxmox
if ! command -v pvesh &> /dev/null; then
    echo -e "${YELLOW}Warning: Not running on Proxmox host${NC}"
    echo "This script assumes Proxmox VE environment"
    echo ""
    read -p "Continue anyway? (y/n) " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        exit 1
    fi
fi

################################################################################
# Phase 1: Create VMs
################################################################################

echo -e "${GREEN}Phase 1: Creating ${VM_COUNT} Virtual Machines${NC}"
echo ""

# VM Specifications
declare -a VM_SPECS
VM_SPECS[1]="id:101,name:bss-db,vcpu:8,ram:32768,disk:200,net:virtio"
VM_SPECS[2]="id:102,name:bss-app,vcpu:12,ram:24576,disk:150,net:virtio"
VM_SPECS[3]="id:103,name:bss-kafka,vcpu:6,ram:16384,disk:200,net:virtio"

for i in {1..3}; do
    SPEC=${VM_SPECS[$i]}
    echo "Creating VM from spec: $SPEC"

    # Parse spec
    VM_ID=$(echo $SPEC | cut -d',' -f1 | cut -d':' -f2)
    VM_NAME=$(echo $SPEC | cut -d',' -f2 | cut -d':' -f2)
    VCPU=$(echo $SPEC | cut -d',' -f3 | cut -d':' -f2)
    RAM=$(echo $SPEC | cut -d',' -f4 | cut -d':' -f2)
    DISK=$(echo $SPEC | cut -d',' -f5 | cut -d':' -f2)

    # Create VM
    if pvesh get /nodes/localhost/lxc | grep -q "CTID: $VM_ID"; then
        echo -e "  ${YELLOW}VM $VM_ID already exists, skipping...${NC}"
    else
        echo -e "  Creating VM $VM_NAME (ID: $VM_ID)..."
        pvesh create /nodes/localhost/lxc \
            --vmid $VM_ID \
            --hostname $VM_NAME \
            --cores $VCPU \
            --memory $RAM \
            --rootfs local:${DISK} \
            --net0 virtio,bridge=vmbr0 \
            --unprivileged 1 \
            --features nesting=1 \
            --onboot 1 \
            --start 1

        # Configure CPU and memory
        pvesh set /nodes/localhost/lxc/$VM_ID \
            --cores $VCPU \
            --memory $RAM

        echo -e "  ${GREEN}âœ“ VM $VM_NAME created${NC}"
    fi
done

echo ""
echo -e "${GREEN}Phase 1 Complete âœ“${NC}"
echo ""

################################################################################
# Phase 2: Install Docker on All VMs
################################################################################

echo -e "${GREEN}Phase 2: Installing Docker & Dependencies${NC}"
echo ""

# Function to install Docker on a VM
install_docker() {
    local VM_ID=$1
    local VM_NAME=$2

    echo "  Installing Docker on $VM_NAME (ID: $VM_ID)..."

    # This would normally be done via SSH or Proxmox API
    # For now, we create scripts to be executed on VMs
    cat > /tmp/install-docker-$VM_ID.sh << EOF
#!/bin/bash
# Docker installation for $VM_NAME

# Update system
apt-get update && apt-get upgrade -y

# Install prerequisites
apt-get install -y apt-transport-https ca-certificates curl gnupg lsb-release

# Add Docker's official GPG key
install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /etc/apt/keyrings/docker.gpg
chmod a+r /etc/apt/keyrings/docker.gpg

# Add Docker repository
echo \\
  "deb [arch=\$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] \\
  https://download.docker.com/linux/ubuntu \$(lsb_release -cs) stable" | \\
  tee /etc/apt/sources.list.d/docker.list > /dev/null

# Install Docker
apt-get update
apt-get install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

# Enable and start Docker
systemctl enable docker
systemctl start docker

# Add current user to docker group (if not root)
if [ "\$USER" != "root" ]; then
    usermod -aG docker \$USER
fi

echo "Docker installed successfully"
EOF

    echo -e "    ${GREEN}âœ“ Docker install script created${NC}"
}

for i in {1..3}; do
    SPEC=${VM_SPECS[$i]}
    VM_ID=$(echo $SPEC | cut -d',' -f1 | cut -d':' -f2)
    VM_NAME=$(echo $SPEC | cut -d',' -f2 | cut -d':' -f2)

    install_docker $VM_ID $VM_NAME
done

echo ""
echo -e "${GREEN}Phase 2 Complete âœ“${NC}"
echo ""

################################################################################
# Phase 3: Create Docker Compose Files
################################################################################

echo -e "${GREEN}Phase 3: Creating Docker Compose Configurations${NC}"
echo ""

# PostgreSQL + Citus Configuration (VM #1)
cat > /tmp/docker-compose-postgres.yml << 'EOF'
version: '3.8'

services:
  postgres-coordinator:
    image: citusdata/citus:12.1.0
    container_name: bss-postgres-coordinator
    environment:
      POSTGRES_USER: bss_app
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: bss
      CITUS_EXTENSION: on
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgresql.conf:/etc/postgresql/postgresql.conf
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U bss_app"]
      interval: 10s
      timeout: 5s
      retries: 5
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G
        reservations:
          cpus: '2'
          memory: 8G

  postgres-worker-1:
    image: citusdata/citus:12.1.0
    container_name: bss-postgres-worker-1
    environment:
      POSTGRES_USER: bss_app
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: bss
      CITUS_EXTENSION: on
    depends_on:
      - postgres-coordinator
    volumes:
      - postgres-worker-1-data:/var/lib/postgresql/data
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G

  postgres-worker-2:
    image: citusdata/citus:12.1.0
    container_name: bss-postgres-worker-2
    environment:
      POSTGRES_USER: bss_app
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: bss
      CITUS_EXTENSION: on
    depends_on:
      - postgres-coordinator
    volumes:
      - postgres-worker-2-data:/var/lib/postgresql/data
    command: >
      postgres
      -c config_file=/etc/postgresql/postgresql.conf
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 16G

  prometheus:
    image: prom/prometheus:latest
    container_name: bss-prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'

volumes:
  postgres-data:
  postgres-worker-1-data:
  postgres-worker-2-data:
EOF

echo -e "  ${GREEN}âœ“ PostgreSQL + Citus compose created${NC}"

# Redis Cluster Configuration (VM #2)
cat > /tmp/docker-compose-redis.yml << 'EOF'
version: '3.8'

services:
  redis-1:
    image: redis:7.2-alpine
    container_name: bss-redis-1
    ports:
      - "6379:6379"
    volumes:
      - redis-1-data:/data
      - ./redis-1.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  redis-2:
    image: redis:7.2-alpine
    container_name: bss-redis-2
    ports:
      - "6380:6379"
    volumes:
      - redis-2-data:/data
      - ./redis-2.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    depends_on:
      - redis-1
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  redis-3:
    image: redis:7.2-alpine
    container_name: bss-redis-3
    ports:
      - "6381:6379"
    volumes:
      - redis-3-data:/data
      - ./redis-3.conf:/usr/local/etc/redis/redis.conf
    command: redis-server /usr/local/etc/redis/redis.conf
    depends_on:
      - redis-1
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  grafana:
    image: grafana/grafana:latest
    container_name: bss-grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - redis-1

volumes:
  redis-1-data:
  redis-2-data:
  redis-3-data:
  grafana-data:
EOF

echo -e "  ${GREEN}âœ“ Redis cluster compose created${NC}"

# Kafka Cluster Configuration (VM #3)
cat > /tmp/docker-compose-kafka.yml << 'EOF'
version: '3.8'

services:
  zookeeper-1:
    image: confluentinc/cp-zookeeper:latest
    container_name: bss-zookeeper-1
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zookeeper-1-data:/var/lib/zookeeper/data
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

  kafka-1:
    image: confluentinc/cp-kafka:7.6.0
    container_name: bss-kafka-1
    depends_on:
      - zookeeper-1
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-1:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 30
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_RETENTION_BYTES: 100000000000
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
    volumes:
      - kafka-1-data:/var/lib/kafka/data
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  kafka-2:
    image: confluentinc/cp-kafka:7.6.0
    container_name: bss-kafka-2
    depends_on:
      - zookeeper-1
    ports:
      - "9093:9092"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-2:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 30
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
    volumes:
      - kafka-2-data:/var/lib/kafka/data
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  kafka-3:
    image: confluentinc/cp-kafka:7.6.0
    container_name: bss-kafka-3
    depends_on:
      - zookeeper-1
    ports:
      - "9094:9092"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper-1:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-3:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_NUM_PARTITIONS: 30
      KAFKA_DEFAULT_REPLICATION_FACTOR: 3
    volumes:
      - kafka-3-data:/var/lib/kafka/data
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G

  kafka-schema-registry:
    image: confluentinc/cp-schema-registry:latest
    container_name: bss-schema-registry
    depends_on:
      - kafka-1
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-1:9092
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 2G

volumes:
  zookeeper-1-data:
  kafka-1-data:
  kafka-2-data:
  kafka-3-data:
EOF

echo -e "  ${GREEN}âœ“ Kafka cluster compose created${NC}"
echo ""

################################################################################
# Phase 4: Create Configuration Files
################################################################################

echo -e "${GREEN}Phase 4: Creating Optimized Configuration Files${NC}"
echo ""

# PostgreSQL Configuration
cat > /tmp/postgresql.conf << 'EOF'
# PostgreSQL 18 Optimized Configuration for 400k events/min

# Connection
max_connections = 500
superuser_reserved_connections = 10

# Memory
shared_buffers = 8GB
effective_cache_size = 24GB
maintenance_work_mem = 2GB
work_mem = 256MB

# Parallelism
max_worker_processes = 8
max_parallel_workers = 8
max_parallel_workers_per_gather = 4

# WAL
wal_level = replica
max_wal_senders = 10
checkpoint_completion_target = 0.9
checkpoint_timeout = 15min
min_wal_size = 4GB
max_wal_size = 16GB
wal_buffers = 16MB

# Logging
log_min_duration_statement = 100
log_checkpoints = on
log_connections = on
log_disconnections = on
log_lock_waits = on

# Performance
shared_preload_libraries = 'citus'
citus.multi_shard_commit_protocol = '2pc'
citus.distributed_table_cache_size = 256

# JIT
jit = on
jit_optimize_above_cost = 1000
jit_inline_above_cost = 5000
jit_decompose_cost = 50000
EOF

echo -e "  ${GREEN}âœ“ PostgreSQL config created${NC}"

# Redis Configuration
cat > /tmp/redis-1.conf << 'EOF'
# Redis 7.2 Optimized Configuration
port 6379
bind 0.0.0.0
protected-mode no

# Memory
maxmemory 4gb
maxmemory-policy allkeys-lru
maxmemory-samples 10

# Persistence
save 900 1
save 300 10
save 60 10000
stop-writes-on-bgsave-error yes
rdbcompression yes
rdbchecksum yes

# Network
tcp-keepalive 300
timeout 300
tcp-backlog 511

# Performance
hash-max-ziplist-entries 512
hash-max-ziplist-value 64
list-max-ziplist-size -2
list-compress-depth 0
set-max-intset-entries 512
zset-max-ziplist-entries 128
zset-max-ziplist-value 64

# Append Only File
appendonly yes
appendfsync everysec
no-appendfsync-on-rewrite no
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 4gb
EOF

echo -e "  ${GREEN}âœ“ Redis config created${NC}"

# Kafka Configuration
cat > /tmp/server-1.properties << 'EOF'
# Kafka 3.7 Server Configuration
broker.id=1
listeners=PLAINTEXT://:9092
advertised.listeners=PLAINTEXT://kafka-1:9092

# Log
log.dirs=/var/lib/kafka/data
log.retention.hours=168
log.segment.bytes=1073741824
log.retention.bytes=100000000000
log.cleanup.policy=delete

# Partition & Replication
num.network.threads=16
num.io.threads=32
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

# Replication
offsets.topic.replication.factor=3
transaction.state.log.replication.factor=3
transaction.state.log.min.isr=2
default.replication.factor=3
min.insync.replicas=2

# Producer
compression.type=snappy
batch.size=65536
linger.ms=10
buffer.memory=33554432
max.in.flight.requests.per.connection=5
retries=3
acks=all
EOF

echo -e "  ${GREEN}âœ“ Kafka config created${NC}"
echo ""

################################################################################
# Phase 5: Create Monitoring Dashboards
################################################################################

echo -e "${GREEN}Phase 5: Creating Monitoring Dashboards${NC}"
echo ""

mkdir -p /tmp/grafana/{dashboards,datasources}

# Prometheus DataSource
cat > /tmp/grafana/datasources/prometheus.yml << 'EOF'
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
EOF

# Grafana Dashboard
cat > /tmp/grafana/dashboards/bss-dashboard.json << 'EOF'
{
  "dashboard": {
    "id": null,
    "title": "BSS - 400k Events/Min Dashboard",
    "tags": ["bss", "events", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Events/Second",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(events_total[1m])",
            "legendFormat": "Events/sec"
          }
        ]
      },
      {
        "id": 2,
        "title": "API Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[1m]))",
            "legendFormat": "P95"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m])",
            "legendFormat": "Error Rate"
          }
        ]
      }
    ]
  }
}
EOF

echo -e "  ${GREEN}âœ“ Grafana dashboard created${NC}"
echo ""

################################################################################
# Phase 6: Create Load Testing Scripts
################################################################################

echo -e "${GREEN}Phase 6: Creating Load Testing Tools${NC}"
echo ""

# K6 Event Generator
cat > /tmp/k6-event-generator.js << 'EOF'
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  scenarios: {
    target_load: {
      executor: 'constant-vus',
      vus: 1250,  // 1250 VUs Ã— ~5 events/VU = ~6,250 events/sec
      duration: '10m',
    },
  },
};

const BASE_URL = __ENV.BASE_URL || 'http://localhost:8080';

export default function () {
  const event = {
    specversion: '1.0',
    type: 'test.event',
    source: 'urn:event-source:k6',
    id: __VU + '-' + __ITER + '-' + Date.now(),
    time: new Date().toISOString(),
    datacontenttype: 'application/json',
    data: {
      message: 'Test event',
      value: Math.random() * 1000,
    },
  };

  const response = http.post(
    `${BASE_URL}/api/v1/events`,
    JSON.stringify(event),
    { headers: { 'Content-Type': 'application/json' } }
  );

  check(response, {
    'status is 202': (r) => r.status === 202,
    'response time < 100ms': (r) => r.timings.duration < 100,
  });

  sleep(1);
}
EOF

# Python Event Simulator
cat > /tmp/simulator.py << 'EOF'
#!/usr/bin/env python3
import asyncio
import json
import time
import random
from datetime import datetime
import aiohttp

async def generate_and_send_events(target_rps: int = 6667, duration: int = 300):
    """
    Generate and send events at target RPS for specified duration
    400,000 events/min = 6,667 events/sec
    """
    print(f"Starting simulator: {target_rps} RPS for {duration} seconds")

    batch_size = 100
    sleep_interval = batch_size / target_rps

    start_time = time.time()
    events_sent = 0

    async with aiohttp.ClientSession() as session:
        while time.time() - start_time < duration:
            # Generate batch of events
            events = []
            for _ in range(batch_size):
                event = {
                    "specversion": "1.0",
                    "type": "test.event",
                    "source": "urn:event-source:simulator",
                    "id": f"{time.time_ns()}-{random.randint(1000, 9999)}",
                    "time": datetime.utcnow().isoformat() + "Z",
                    "datacontenttype": "application/json",
                    "data": {
                        "value": random.uniform(0, 1000),
                        "category": random.choice(["A", "B", "C"]),
                    },
                }
                events.append(event)

            # Send batch
            tasks = [
                session.post(
                    'http://localhost:8080/api/v1/events',
                    json=event,
                    headers={'Content-Type': 'application/json'}
                )
                for event in events
            ]

            responses = await asyncio.gather(*tasks, return_exceptions=True)
            successful = sum(1 for r in responses if not isinstance(r, Exception))
            events_sent += successful

            elapsed = time.time() - start_time
            current_rps = events_sent / elapsed if elapsed > 0 else 0

            print(f"Events sent: {events_sent:,} | Current RPS: {current_rps:.2f}")

            await asyncio.sleep(sleep_interval)

    print(f"\nSimulation complete!")
    print(f"Total events: {events_sent:,}")
    print(f"Average RPS: {events_sent / duration:.2f}")

if __name__ == "__main__":
    asyncio.run(generate_and_send_events())
EOF

chmod +x /tmp/simulator.py

echo -e "  ${GREEN}âœ“ K6 generator created${NC}"
echo -e "  ${GREEN}âœ“ Python simulator created${NC}"
echo ""

################################################################################
# Phase 7: Create Deployment Scripts
################################################################################

echo -e "${GREEN}Phase 7: Creating Deployment Scripts${NC}"
echo ""

# Master deployment script
cat > /tmp/deploy-all.sh << 'EOF'
#!/bin/bash

echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
echo "â•‘  BSS 400k Events/Min - Deploy All    â•‘"
echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

# Deploy PostgreSQL (VM #1)
echo "Deploying PostgreSQL + Citus..."
ssh root@bss-db-vm "cd /opt/bss && docker-compose -f docker-compose-postgres.yml up -d"
echo "âœ“ PostgreSQL deployed"

# Deploy Redis (VM #2)
echo "Deploying Redis cluster..."
ssh root@bss-app-vm "cd /opt/bss && docker-compose -f docker-compose-redis.yml up -d"
echo "âœ“ Redis deployed"

# Deploy Kafka (VM #3)
echo "Deploying Kafka cluster..."
ssh root@bss-kafka-vm "cd /opt/bss && docker-compose -f docker-compose-kafka.yml up -d"
echo "âœ“ Kafka deployed"

echo ""
echo "All services deployed successfully!"
echo ""
echo "Next steps:"
echo "  1. Check service health: docker ps"
echo "  2. Run load test: k6 run k6-event-generator.js"
echo "  3. View dashboard: http://localhost:3000"
echo ""
echo "Default credentials:"
echo "  Grafana: admin / admin"
echo "  Prometheus: http://localhost:9090"
EOF

chmod +x /tmp/deploy-all.sh

echo -e "  ${GREEN}âœ“ Deployment script created${NC}"
echo ""

################################################################################
# Final Summary
################################################################################

echo -e "${GREEN}â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—${NC}"
echo -e "${GREEN}â•‘            QUICK START SETUP COMPLETE âœ“                   â•‘${NC}"
echo -e "${GREEN}â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo ""
echo -e "${BLUE}What was created:${NC}"
echo "  âœ“ 3 Virtual Machine configurations"
echo "  âœ“ Docker Compose files for all services"
echo "  âœ“ Optimized configuration files (PostgreSQL, Redis, Kafka)"
echo "  âœ“ Monitoring dashboard (Grafana + Prometheus)"
echo "  âœ“ Load testing scripts (K6 + Python)"
echo "  âœ“ Deployment automation script"
echo ""
echo -e "${YELLOW}Target Architecture:${NC}"
echo "  VM #1 (bss-db): PostgreSQL 18 + Citus (8 vCPU, 32GB RAM, 200GB SSD)"
echo "  VM #2 (bss-app): Redis 7.2 Cluster (12 vCPU, 24GB RAM, 150GB SSD)"
echo "  VM #3 (bss-kafka): Kafka 3.7 Cluster (6 vCPU, 16GB RAM, 200GB SSD)"
echo ""
echo -e "${YELLOW}Expected Performance:${NC}"
echo "  â€¢ ${TARGET_EVENTS_PER_MIN:,} events/minute (${TARGET_EVENTS_PER_SEC} events/second)"
echo "  â€¢ 99.9% uptime SLA"
echo "  â€¢ Sub-100ms latency P95"
echo "  â€¢ Automatic horizontal scaling"
echo ""
echo -e "${BLUE}Next Steps:${NC}"
echo "  1. Copy files to VMs:"
echo "     - /tmp/docker-compose-*.yml"
echo "     - /tmp/*.conf"
echo "     - /tmp/*.js"
echo "     - /tmp/*.py"
echo ""
echo "  2. On each VM, run:"
echo "     ssh root@<vm-ip>"
echo "     docker-compose up -d"
echo ""
echo "  3. Start load testing:"
echo "     k6 run k6-event-generator.js"
echo "     OR"
echo "     python3 simulator.py"
echo ""
echo -e "${GREEN}For detailed information, see:${NC}"
echo "  /home/labadmin/projects/droid-spring/ARCHITEKTURA_SKALOWALNA_400K_EVENTS_MINUTA.md"
echo ""
echo -e "${BLUE}Happy scaling! ðŸš€${NC}"
echo ""
