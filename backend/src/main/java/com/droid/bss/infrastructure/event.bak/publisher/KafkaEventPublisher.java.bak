package com.droid.bss.infrastructure.event.publisher;

import io.cloudevents.CloudEvent;
import io.cloudevents.core.builder.CloudEventBuilder;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;
import org.springframework.kafka.core.KafkaTemplate;
import org.springframework.kafka.support.SendResult;
import org.springframework.kafka.support.mapping.DomainClassMapper;
import org.springframework.kafka.support.mapping.DomainClassMapperHolder;
import org.springframework.scheduling.annotation.Async;
import org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor;

import java.net.URI;
import java.time.Duration;
import java.time.Instant;
import java.time.OffsetDateTime;
import java.util.*;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.TimeUnit;
import java.util.stream.Collectors;

/**
 * Kafka-based implementation of DomainEventPublisher.
 *
 * Uses CloudEvents v1.0 specification for interoperability.
 *
 * @since 1.0
 */
public class KafkaEventPublisher implements DomainEventPublisher {

    private static final Logger log = LoggerFactory.getLogger(KafkaEventPublisher.class);

    private final KafkaTemplate<String, Object> kafkaTemplate;
    private final ObjectMapper objectMapper;
    private final EventProperties eventProperties;
    private final ThreadPoolTaskExecutor taskExecutor;
    private final Map<String, EventDeduplicationCache> deduplicationCaches;

    /**
     * Creates a new KafkaEventPublisher.
     *
     * @param kafkaTemplate the Kafka template
     * @param objectMapper the JSON object mapper
     * @param eventProperties the event properties
     */
    public KafkaEventPublisher(KafkaTemplate<String, Object> kafkaTemplate,
                               ObjectMapper objectMapper,
                               EventProperties eventProperties) {
        this.kafkaTemplate = kafkaTemplate;
        this.objectMapper = objectMapper;
        this.eventProperties = eventProperties;
        this.taskExecutor = createTaskExecutor();
        this.deduplicationCaches = new ConcurrentHashMap<>();
    }

    /**
     * Creates a new KafkaEventPublisher without event properties.
     *
     * @param kafkaTemplate the Kafka template
     * @param objectMapper the JSON object mapper
     */
    public KafkaEventPublisher(KafkaTemplate<String, Object> kafkaTemplate,
                               ObjectMapper objectMapper) {
        this(kafkaTemplate, objectMapper, new EventProperties());
    }

    /**
     * No-args constructor for tests.
     */
    public KafkaEventPublisher() {
        this.kafkaTemplate = null;
        this.objectMapper = null;
        this.eventProperties = new EventProperties();
        this.taskExecutor = createTaskExecutor();
        this.deduplicationCaches = new ConcurrentHashMap<>();
    }

    @Override
    public void publish(DomainEvent event) {
        if (event == null) {
            throw EventPublishingException.invalidEvent("Event cannot be null");
        }

        try {
            if (!eventProperties.getEnabled()) {
                log.debug("Event publishing is disabled, skipping event: {}", event.getId());
                return;
            }

            String topic = getTopicName(event);
            String key = getMessageKey(event);

            // Convert to CloudEvent
            CloudEvent cloudEvent = convertToCloudEvent(event);

            // Publish to Kafka
            CompletableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, key, cloudEvent);

            // Wait for result (synchronous)
            SendResult<String, Object> result = future.get(eventProperties.getAsyncTimeout().toMillis(), TimeUnit.MILLISECONDS);

            log.debug("Event published successfully: topic={}, eventId={}, partition={}, offset={}",
                topic,
                event.getId(),
                result.getRecordMetadata().partition(),
                result.getRecordMetadata().offset()
            );

        } catch (EventPublishingException e) {
            throw e;
        } catch (Exception e) {
            throw EventPublishingException.kafkaDeliveryFailure(
                event.getId(),
                event.getType(),
                getTopicName(event),
                e
            );
        }
    }

    @Override
    @Async
    public CompletableFuture<EventPublishResult> publishAsync(DomainEvent event) {
        if (event == null) {
            return CompletableFuture.completedFuture(
                EventPublishResult.failure(null, null, "Event cannot be null")
            );
        }

        if (!eventProperties.getEnabled()) {
            return CompletableFuture.completedFuture(
                EventPublishResult.failure(event.getId(), getTopicName(event), "Event publishing is disabled")
            );
        }

        return CompletableFuture.supplyAsync(() -> {
            try {
                String topic = getTopicName(event);
                String key = getMessageKey(event);
                CloudEvent cloudEvent = convertToCloudEvent(event);

                CompletableFuture<SendResult<String, Object>> future = kafkaTemplate.send(topic, key, cloudEvent);

                SendResult<String, Object> result = future.get(eventProperties.getAsyncTimeout().toMillis(), TimeUnit.MILLISECONDS);

                return EventPublishResult.success(
                    event.getId(),
                    topic,
                    Instant.now(),
                    event.getPartitionKey(),
                    result.getRecordMetadata().partition(),
                    result.getRecordMetadata().offset()
                );

            } catch (Exception e) {
                log.error("Failed to publish event asynchronously: {}", event.getId(), e);
                return EventPublishResult.failure(
                    event.getId(),
                    getTopicName(event),
                    e.getMessage(),
                    Instant.now()
                );
            }
        }, taskExecutor);
    }

    @Override
    public void publishBatch(DomainEvent... events) {
        if (events == null || events.length == 0) {
            log.warn("No events to publish");
            return;
        }

        if (!eventProperties.getEnabled()) {
            log.debug("Event publishing is disabled, skipping {} events", events.length);
            return;
        }

        try {
            // Group events by topic for batch publishing
            Map<String, List<CloudEvent>> eventsByTopic = Arrays.stream(events)
                .filter(Objects::nonNull)
                .map(this::convertToCloudEvent)
                .collect(Collectors.groupingBy(
                    event -> getTopicNameFromCloudEvent(event),
                    LinkedHashMap::new,
                    Collectors.toList()
                ));

            // Publish to each topic
            for (Map.Entry<String, List<CloudEvent>> entry : eventsByTopic.entrySet()) {
                String topic = entry.getKey();
                List<CloudEvent> topicEvents = entry.getValue();

                // Publish events to topic
                List<CompletableFuture<SendResult<String, Object>>> futures = topicEvents.stream()
                    .map(event -> {
                        String key = event.getExtension("partitionKey") != null ?
                            String.valueOf(event.getExtension("partitionKey")) : null;
                        return kafkaTemplate.send(topic, key, event);
                    })
                    .collect(Collectors.toList());

                // Wait for all futures to complete
                CompletableFuture<Void> allFutures = CompletableFuture.allOf(
                    futures.toArray(new CompletableFuture[0])
                );

                allFutures.get(eventProperties.getAsyncTimeout().toMillis(), TimeUnit.MILLISECONDS);

                log.debug("Batch published to topic: topic={}, count={}", topic, topicEvents.size());
            }

        } catch (Exception e) {
            throw EventPublishingException.batchPublishingFailure(events.length, 0, e);
        }
    }

    @Override
    @Async
    public CompletableFuture<EventBatchPublishResult> publishBatchAsync(DomainEvent... events) {
        if (events == null || events.length == 0) {
            return CompletableFuture.completedFuture(
                EventBatchPublishResult.failure(0, Collections.emptyList(), Duration.ZERO)
            );
        }

        if (!eventProperties.getEnabled()) {
            List<EventPublishResult> results = Arrays.stream(events)
                .filter(Objects::nonNull)
                .map(event -> EventPublishResult.failure(
                    event.getId(),
                    getTopicName(event),
                    "Event publishing is disabled"
                ))
                .collect(Collectors.toList());

            return CompletableFuture.completedFuture(
                EventBatchPublishResult.failure(events.length, results, Duration.ZERO)
            );
        }

        return CompletableFuture.supplyAsync(() -> {
            Instant startTime = Instant.now();
            List<EventPublishResult> results = new ArrayList<>();

            try {
                // Publish each event and collect results
                for (DomainEvent event : events) {
                    if (event == null) {
                        continue;
                    }

                    try {
                        publish(event);
                        results.add(EventPublishResult.success(
                            event.getId(),
                            getTopicName(event),
                            Instant.now(),
                            event.getPartitionKey(),
                            0,
                            0
                        ));
                    } catch (Exception e) {
                        results.add(EventPublishResult.failure(
                            event.getId(),
                            getTopicName(event),
                            e.getMessage(),
                            Instant.now()
                        ));
                    }
                }

                Duration processingTime = Duration.between(startTime, Instant.now());

                boolean allSuccess = results.stream().allMatch(EventPublishResult::isSuccess);
                if (allSuccess) {
                    return EventBatchPublishResult.success(results, processingTime);
                } else {
                    return EventBatchPublishResult.failure(events.length, results, processingTime);
                }

            } catch (Exception e) {
                Duration processingTime = Duration.between(startTime, Instant.now());
                log.error("Batch publishing failed for {} events", events.length, e);
                return EventBatchPublishResult.failure(events.length, results, processingTime);
            }
        }, taskExecutor);
    }

    @Override
    public void flush() {
        if (kafkaTemplate != null) {
            kafkaTemplate.flush();
            log.debug("Event publisher flushed");
        }
    }

    @Override
    public boolean isReady() {
        // Check if Kafka template is available
        if (kafkaTemplate == null) {
            return false;
        }

        // Check if publishing is enabled
        if (!eventProperties.getEnabled()) {
            return false;
        }

        // Check if task executor is active
        if (taskExecutor != null && taskExecutor.getActiveCount() >= taskExecutor.getMaxPoolSize()) {
            return false;
        }

        return true;
    }

    @Override
    public String getName() {
        return "KafkaEventPublisher";
    }

    // Private helper methods

    private CloudEvent convertToCloudEvent(DomainEvent event) {
        try {
            CloudEventBuilder builder = CloudEventBuilder.v1()
                .withId(event.getId())
                .withType(event.getType())
                .withSource(URI.create(event.getSource()))
                .withTime(OffsetDateTime.from(event.getTime().atZone(java.time.ZoneId.systemDefault())))
                .withData("application/json", objectMapper.writeValueAsBytes(event));

            if (event.getSchemaUrl() != null) {
                builder = builder.withSchemaUrl(event.getSchemaUrl());
            }

            if (event.getSubject() != null) {
                builder = builder.withSubject(event.getSubject());
            }

            if (event.getPartitionKey() != null) {
                builder = builder.withExtension("partitionKey", event.getPartitionKey());
            }

            return builder.build();

        } catch (Exception e) {
            throw EventPublishingException.serializationFailure(
                event.getId(),
                event.getType(),
                getTopicName(event),
                e.getMessage()
            );
        }
    }

    private String getTopicName(DomainEvent event) {
        String eventType = event.getType().replace('.', '-');
        return eventProperties.getTopicPrefix() + "." + eventType;
    }

    private String getTopicNameFromCloudEvent(CloudEvent event) {
        String eventType = event.getType().replace('.', '-');
        return eventProperties.getTopicPrefix() + "." + eventType;
    }

    private String getMessageKey(DomainEvent event) {
        // Use partition key if available, otherwise use event ID
        return event.getPartitionKey() != null ? event.getPartitionKey() : event.getId();
    }

    private ThreadPoolTaskExecutor createTaskExecutor() {
        ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor();
        executor.setCorePoolSize(eventProperties.getParallelism());
        executor.setMaxPoolSize(eventProperties.getParallelism() * 2);
        executor.setQueueCapacity(eventProperties.getBufferSize());
        executor.setThreadNamePrefix("event-publisher-");
        executor.setWaitForTasksToCompleteOnShutdown(true);
        executor.initialize();
        return executor;
    }

    /**
     * Event deduplication cache for preventing duplicate events.
     */
    private static class EventDeduplicationCache {
        private final Map<String, Long> eventIds = new ConcurrentHashMap<>();
        private final long ttlMillis;

        EventDeduplicationCache(Duration ttl) {
            this.ttlMillis = ttl.toMillis();
        }

        boolean isDuplicate(String eventId) {
            Long lastSeen = eventIds.get(eventId);
            if (lastSeen != null) {
                // Check if event ID has been seen recently
                long now = System.currentTimeMillis();
                return (now - lastSeen) < ttlMillis;
            }
            // Mark event ID as seen
            eventIds.put(eventId, System.currentTimeMillis());
            return false;
        }

        void cleanup() {
            long now = System.currentTimeMillis();
            eventIds.entrySet().removeIf(entry -> (now - entry.getValue()) >= ttlMillis);
        }
    }
}
